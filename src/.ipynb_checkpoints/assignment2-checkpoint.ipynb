{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the audio file as a timeseries in y and store sampling rate as sr(measured in Hz)\n",
    "default sr is 22kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(path):\n",
    "    y, sr = librosa.load(path)\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    \n",
    "    features = []\n",
    "    features.append(tempo)\n",
    "    features.append(np.sum(beats))\n",
    "    features.append(np.mean(chroma_stft))\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.mean(cent))\n",
    "    features.append(np.mean(spec_bw))\n",
    "    features.append(np.mean(rolloff))\n",
    "    features.append(np.mean(zcr))\n",
    "    for coefficient in mfcc:\n",
    "        features.append(np.mean(coefficient))\n",
    "    return features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "1. Our training dataset can be found [here](https://iiitaphyd-my.sharepoint.com/personal/devansh_manu_research_iiit_ac_in/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly9paWl0YXBoeWQtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvZGV2YW5zaF9tYW51X3Jlc2VhcmNoX2lpaXRfYWNfaW4vRWxZZm1zQ3h0Q2xJcjJLSVBqcWEzYUFCdFVva2xha2cwNnBCMDJMMmlKaWVIdz9ydGltZT1UNjRHc1lqQzEwZw&viewid=5db72c70%2D223f%2D4887%2Dbf50%2Dbd0fbf14638f&id=%2Fpersonal%2Fdevansh%5Fmanu%5Fresearch%5Fiiit%5Fac%5Fin%2FDocuments%2FMMT%20%2D%20Hands%20on%20activity%2FEmotion%20Classification)\n",
    "2. From the above link, we picked up the Arousal and Valence data.\n",
    "3. From [this](https://onedrive.live.com/?authkey=%21ABJMt2rGTQvCxyM&id=3E1ACB43A24F0BDA%21352&cid=3E1ACB43A24F0BDA) link, we picked up the already-annotated Tension data.\n",
    "4. Training of our model was done on the above mentioned Arousal, Valence and Tension dataset. \n",
    "5. Our testing, which you would see at the end of this notebook, is done on the un-annotated data mentioned in this [link](https://onedrive.live.com/?authkey=%21ABJMt2rGTQvCxyM&id=3E1ACB43A24F0BDA%21122&cid=3E1ACB43A24F0BDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training data-directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_set, y_labels = [], []\n",
    "dir_list = [\"../emotion_dataset/training/Arousal/\", \"../emotion_dataset/training/Valence/\", \"../emotion_dataset/training/Tension/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features for Arousal and Valence music tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    dirName1, dirName2 = dir_list[i]+\"Positive/\", dir_list[i]+\"Negative/\"\n",
    "    dirContents = os.listdir(dirName1)\n",
    "    temp_y_label = \"A\"\n",
    "    if i == 1:\n",
    "        temp_y_label = \"V\"\n",
    "    for music in dirContents:\n",
    "        currSongFeatures = getFeatures(dirName1+music)\n",
    "        y_labels.append(temp_y_label+\"P\")\n",
    "        feature_set.append(currSongFeatures)\n",
    "    dirContents = os.listdir(dirName2)    \n",
    "    for music in dirContents:\n",
    "        currSongFeatures = getFeatures(dirName2+music)\n",
    "        y_labels.append(temp_y_label+\"N\")\n",
    "        feature_set.append(currSongFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This <font color=\"red\">warning</font> will always occur when loading mp3 because libsndfile does not (yet/currently) support the mp3 format. Librosa tries to use libsndfile first, and if that fails, it will fall back on the audioread package, which is a bit slower and more brittle, but supports more formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types of labels gathered uptill now, i.e. Arousal and Valence(both can be positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = dir_list[-1]\n",
    "dirContents = os.listdir(dirName)\n",
    "for music in dirContents:\n",
    "    temp_y_label = music.split(\"_\")[-1][:2]\n",
    "    if temp_y_label == \"TN\" or temp_y_label == \"TP\":\n",
    "        currSongFeatures = getFeatures(dirName+music)\n",
    "        feature_set.append(currSongFeatures)\n",
    "        y_labels.append(temp_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_labels))\n",
    "print(len(feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = np.array(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y_labels)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of classes in our entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the training dataset is itself heavily biased w.r.t. the classes TN and TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y, return_counts=True)\n",
    "print(\"Classes are: \", uniqueValues)\n",
    "print(\"Frequency of each class is: \", occurCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numExamplesPerClass = min(occurCount)\n",
    "numExamplesPerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y_labels[:81]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_labels = y_labels[:81]\n",
    "new_feature_set = feature_set[:81]\n",
    "print(len(new_y_labels))\n",
    "print(len(new_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_count, tp_count, vn_count, vp_count, an_count, ap_count = 0, 0, 0, 0, 0, 0\n",
    "new_y_labels = []\n",
    "new_feature_set = []\n",
    "for l in range(len(y_labels)):\n",
    "    if y_labels[l] == 'TN' and tn_count < numExamplesPerClass:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set.append(feature_set[l])\n",
    "        tn_count += 1\n",
    "    if y_labels[l] == 'TP' and tp_count < numExamplesPerClass:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set.append(feature_set[l])\n",
    "        tp_count += 1\n",
    "    if y_labels[l] == 'AP' and ap_count < numExamplesPerClass:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set.append(feature_set[l])\n",
    "        ap_count += 1\n",
    "    if y_labels[l] == 'AN' and an_count < numExamplesPerClass:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set.append(feature_set[l])\n",
    "        an_count += 1\n",
    "    if y_labels[l] == 'VN' and vn_count < numExamplesPerClass:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set.append(feature_set[l])\n",
    "        vn_count += 1\n",
    "    if y_labels[l] == 'VP' and vp_count < numExamplesPerClass:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set.append(feature_set[l])\n",
    "        vp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_count, tp_count = 0, 0\n",
    "for l in range(81, len(y_labels)):\n",
    "    if y_labels[l] == 'TN' and tn_count < 20:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set = np.append(new_feature_set,[feature_set[l]], axis=0)\n",
    "        tn_count += 1\n",
    "    if y_labels[l] == 'TP' and tp_count < 20:\n",
    "        new_y_labels.append(y_labels[l])\n",
    "        new_feature_set = np.append(new_feature_set,[feature_set[l]], axis=0)\n",
    "        tp_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(new_y_labels))\n",
    "print(len(new_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_set = np.array(new_feature_set)\n",
    "new_feature_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(new_y_labels)\n",
    "encoded_Y = encoder.transform(new_y_labels)\n",
    "Y = to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['tempo', 'beats', 'chromagram', 'rmse',\n",
    "           'centroid', 'bandwidth', 'rolloff', 'zcr', 'mfcc1', 'mfcc2',\n",
    "           'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9',\n",
    "           'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15',\n",
    "           'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, nrows=7)\n",
    "\n",
    "for i, ax in zip(range(28), axes.flat):\n",
    "    sns.distplot(new_feature_set[:,i], hist=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model5: Neural Net\n",
    "Use self-normalised exponential units as activation for all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(128, input_shape=(new_feature_set.shape[-1],), activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal))\n",
    "model5.add(Dense(128*2, activation='relu', kernel_initializer='normal'))\n",
    "model5.add(Dense(128*4, activation='relu', kernel_initializer='normal'))\n",
    "model5.add(Dense(128*8, activation='relu', kernel_initializer='normal'))\n",
    "model5.add(Dense(128*16, activation='relu', kernel_initializer='normal'))\n",
    "model5.add(Dense(128*32, activation='relu', kernel_initializer='normal'))\n",
    "model5.add(Dense(128*64, activation='relu', kernel_initializer='normal'))\n",
    "model5.add(Dense(128*128, activation='relu', kernel_initializer='normal'))    \n",
    "model5.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(new_feature_set, Y, test_size=0.15, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = model5.fit(x_train, y_train, batch_size=16, validation_data=[x_val, y_val], callbacks=[es], epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 5 predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the testing data as mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_directory = \"../emotion_dataset/testing/\"\n",
    "dirContents = os.listdir(testing_directory)\n",
    "x_test, y_test = [], []\n",
    "for music in dirContents:\n",
    "    labelled_music = music.split('_')[1]\n",
    "    if labelled_music[1] == 'P' or labelled_music[1] == 'N':\n",
    "        music_features = getFeatures(testing_directory+music)\n",
    "        x_test.append(music_features)\n",
    "        y_test.append(labelled_music[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_i = x_test[i].reshape(1, x_test[i].shape[0])\n",
    "    y_i_pred = model5.predict(x_i)\n",
    "    y_i_pred = y_i_pred.reshape(y_i_pred.shape[-1],)\n",
    "    y_i_pred = encoder.inverse_transform([np.argmax(y_i_pred)])\n",
    "    y_pred.append(y_i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(map(lambda x: x[0], y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(map(lambda x: x[0], y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(x_val.shape[0]):\n",
    "    x_i = x_val[i].reshape(1, x_val[i].shape[0])\n",
    "    y_i_pred = model5.predict(x_i)\n",
    "    y_i_pred = y_i_pred.reshape(y_i_pred.shape[-1],)\n",
    "    y_i_pred = encoder.inverse_transform([np.argmax(y_i_pred)])\n",
    "    y_pred.append(y_i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labelled = []\n",
    "for i in y_val:\n",
    "    t = encoder.inverse_transform([np.argmax(i)])\n",
    "    y_val_labelled.append(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_val_labelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_val_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list(map(lambda x: x[0], y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val_labelled, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = gmm.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = result6.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(x_val.shape[0]):\n",
    "    x_i = x_val[i].reshape(1, x_val[i].shape[0])\n",
    "    y_i_pred = rf.predict(x_i)\n",
    "    y_i_pred = y_i_pred.reshape(y_i_pred.shape[-1],)\n",
    "    y_i_pred = encoder.inverse_transform([np.argmax(y_i_pred)])\n",
    "    y_pred.append(y_i_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labelled = []\n",
    "for i in y_val:\n",
    "    t = encoder.inverse_transform([np.argmax(i)])\n",
    "    y_val_labelled.append(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_val_labelled, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_val_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
