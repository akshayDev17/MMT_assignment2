{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the audio file as a timeseries in y and store sampling rate as sr(measured in Hz)\n",
    "default sr is 22kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(path):\n",
    "    y, sr = librosa.load(path)\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    \n",
    "    features = []\n",
    "    features.append(tempo)\n",
    "    features.append(np.sum(beats))\n",
    "    features.append(np.mean(chroma_stft))\n",
    "    features.append(np.mean(rms))\n",
    "    features.append(np.mean(cent))\n",
    "    features.append(np.mean(spec_bw))\n",
    "    features.append(np.mean(rolloff))\n",
    "    features.append(np.mean(zcr))\n",
    "    for coefficient in mfcc:\n",
    "        features.append(np.mean(coefficient))\n",
    "    return features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "1. Our training dataset can be found [here](https://iiitaphyd-my.sharepoint.com/personal/devansh_manu_research_iiit_ac_in/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly9paWl0YXBoeWQtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvZGV2YW5zaF9tYW51X3Jlc2VhcmNoX2lpaXRfYWNfaW4vRWxZZm1zQ3h0Q2xJcjJLSVBqcWEzYUFCdFVva2xha2cwNnBCMDJMMmlKaWVIdz9ydGltZT1UNjRHc1lqQzEwZw&viewid=5db72c70%2D223f%2D4887%2Dbf50%2Dbd0fbf14638f&id=%2Fpersonal%2Fdevansh%5Fmanu%5Fresearch%5Fiiit%5Fac%5Fin%2FDocuments%2FMMT%20%2D%20Hands%20on%20activity%2FEmotion%20Classification)\n",
    "2. From the above link, we picked up the Arousal and Valence data.\n",
    "3. From [this](https://onedrive.live.com/?authkey=%21ABJMt2rGTQvCxyM&id=3E1ACB43A24F0BDA%21352&cid=3E1ACB43A24F0BDA) link, we picked up the already-annotated Tension data.\n",
    "4. Training of our model was done on the above mentioned Arousal, Valence and Tension dataset. \n",
    "5. Our testing, which you would see at the end of this notebook, is done on the un-annotated data mentioned in this [link](https://onedrive.live.com/?authkey=%21ABJMt2rGTQvCxyM&id=3E1ACB43A24F0BDA%21122&cid=3E1ACB43A24F0BDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get training data-directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_set, y_labels = [], []\n",
    "dir_list = [\"../emotion_dataset/training/Arousal/\", \"../emotion_dataset/training/Valence/\", \"../emotion_dataset/training/Tension/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features for Arousal and Valence music tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    dirName1, dirName2 = dir_list[i]+\"Positive/\", dir_list[i]+\"Negative/\"\n",
    "    dirContents = os.listdir(dirName1)\n",
    "    temp_y_label = \"A\"\n",
    "    if i == 1:\n",
    "        temp_y_label = \"V\"\n",
    "    for music in dirContents:\n",
    "        currSongFeatures = getFeatures(dirName1+music)\n",
    "        y_labels.append(temp_y_label+\"P\")\n",
    "        feature_set.append(currSongFeatures)\n",
    "    dirContents = os.listdir(dirName2)    \n",
    "    for music in dirContents:\n",
    "        currSongFeatures = getFeatures(dirName2+music)\n",
    "        y_labels.append(temp_y_label+\"N\")\n",
    "        feature_set.append(currSongFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This <font color=\"red\">warning</font> will always occur when loading mp3 because libsndfile does not (yet/currently) support the mp3 format. Librosa tries to use libsndfile first, and if that fails, it will fall back on the audioread package, which is a bit slower and more brittle, but supports more formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types of labels gathered uptill now, i.e. Arousal and Valence(both can be positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = dir_list[-1]\n",
    "dirContents = os.listdir(dirName)\n",
    "for music in dirContents:\n",
    "    temp_y_label = music.split(\"_\")[-1][:2]\n",
    "    if temp_y_label == \"TN\" or temp_y_label == \"TP\":\n",
    "        currSongFeatures = getFeatures(dirName+music)\n",
    "        feature_set.append(currSongFeatures)\n",
    "        y_labels.append(temp_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_labels))\n",
    "print(len(feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = np.array(feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y_labels)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['tempo', 'beats', 'chromagram', 'rmse',\n",
    "           'centroid', 'bandwidth', 'rolloff', 'zcr', 'mfcc1', 'mfcc2',\n",
    "           'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9',\n",
    "           'mfcc10', 'mfcc11', 'mfcc12', 'mfcc13', 'mfcc14', 'mfcc15',\n",
    "           'mfcc16', 'mfcc17', 'mfcc18', 'mfcc19', 'mfcc20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(feature_set[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model1:\n",
    "this uses categorical crossentropy as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(128, input_shape=(feature_set.shape[1],), activation='relu'))\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "model1.add(Dense(1024, activation='relu'))\n",
    "model1.add(Dense(2048, activation='relu'))\n",
    "model1.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_labels)\n",
    "encoded_Y = encoder.transform(y_labels)\n",
    "Y = to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(feature_set, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of training data is: \", x_train.shape)\n",
    "print(\"shape of training label is: \", y_train.shape)\n",
    "print(\"shape of testing data is: \", x_test.shape)\n",
    "print(\"shape of testing label is: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model1.fit(x_train, y_train, epochs=20, callbacks=[es],validation_data=([x_val, y_val]), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'], label='train')\n",
    "plt.plot(results.history['val_loss'], label='val')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(\"../emotion_dataset/Tension/20161058_TN2.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2:\n",
    "Here I try to add 1 extra layer(dense_4 with 1024 x 1), but with a lesser dimension as its previous layer(dense_3 with 2048 x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(128, input_shape=(feature_set.shape[1],), activation='relu'))\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dense(2048, activation='relu'))\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dense(6, activation='softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_set, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = model2.fit(x_train, y_train, callbacks=[es], batch_size=16, validation_data=[x_test, y_test], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'], label='train_model1')\n",
    "plt.plot(result2.history['loss'], label='train_model2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['val_loss'], label='test_model1')\n",
    "plt.plot(result2.history['val_loss'], label='test_model2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(32, input_shape=(feature_set.shape[1],), activation='relu'))\n",
    "model3.add(Dense(64, activation='tanh'))\n",
    "model3.add(Dense(128, activation='tanh'))\n",
    "model3.add(Dense(256, activation=tf.nn.sigmoid))\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_set, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result3 = model3.fit(x_train, y_train, callbacks=[es], batch_size=8, validation_data=[x_test, y_test], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'], label='train_model1')\n",
    "plt.plot(result2.history['loss'], label='train_model2')\n",
    "plt.plot(result3.history['loss'], label='train_model3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['val_loss'], label='test_model1')\n",
    "plt.plot(result2.history['val_loss'], label='test_model2')\n",
    "plt.plot(result3.history['val_loss'], label='test_model3')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(32, input_shape=(feature_set.shape[1],), activation='relu'))\n",
    "model4.add(Dense(64, activation='tanh'))\n",
    "model4.add(Dense(128, activation='tanh'))\n",
    "model4.add(Dense(256, activation=tf.nn.sigmoid))\n",
    "model4.add(Dense(512, activation='relu'))\n",
    "model4.add(Dense(2048, activation=tf.keras.activations.tanh))\n",
    "model4.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_set, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = model4.fit(x_train, y_train, epochs=20, validation_data=[x_test, y_test], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result3.history['loss'], label='train3_loss')\n",
    "plt.plot(result4.history['loss'], label='train4_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result3.history['val_loss'], label='test3_loss')\n",
    "plt.plot(result4.history['val_loss'], label='test4_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = getFeatures(\"../emotion_dataset/Tension/20161058_TN2.mp3\")\n",
    "x_1 = np.array(x_1)\n",
    "x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x_1.reshape(1,x_1.shape[0])\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_model3_pred = model3.predict(x1)\n",
    "y1_model3_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_model3_pred = y1_model3_pred.reshape(y1_model3_pred.shape[-1],)\n",
    "print(y1_model3_pred.shape)\n",
    "y1_model3_pred = encoder.inverse_transform([np.argmax(y1_model3_pred)])\n",
    "print(y1_model3_pred)\n",
    "y1_true = ['TN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_true, y_pred)\n",
    "accuracy_score(y1_true, y1_model3_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
